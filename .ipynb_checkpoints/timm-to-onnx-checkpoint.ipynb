{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81cdcd5a-fd74-4bb9-9ad8-af94de238e2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -Uq timm onnx onnxruntime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccb9bbd-f87e-4573-b8b8-2003946ab517",
   "metadata": {},
   "source": [
    "## PyTorch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8d290ce0-e6b6-4724-9223-7668fed22f22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "import timm\n",
    "import torch\n",
    "\n",
    "img = Image.open(urlopen(\n",
    "    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n",
    "))\n",
    "\n",
    "model = timm.create_model('efficientvit_b0.r224_in1k', pretrained=True)\n",
    "model = model.eval()\n",
    "\n",
    "# get model specific transforms (normalization, resize)\n",
    "data_config = timm.data.resolve_model_data_config(model)\n",
    "transforms = timm.data.create_transform(**data_config, is_training=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7a376225-5843-47e2-80f6-9b7ac8ebe3ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.9 ms ± 498 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c7b4dbc4-25f0-40ae-90cf-a11245a02d08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5e3105dd-7417-4f9a-99b1-b044c9527ecc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[42.1539,  9.7981,  9.3126,  5.4653,  4.5588]],\n",
       "       grad_fn=<TopkBackward0>)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "cc7270a1-c284-456e-9277-2681fe68a2b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[928, 551, 969, 967, 505]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5_class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1166b0-e6fd-4c13-8e27-78b5637196ea",
   "metadata": {},
   "source": [
    "## Convert To ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "34b09639-41ee-4f4f-971b-f9bf2b5ced51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from timm.utils.model import reparameterize_model\n",
    "model = reparameterize_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1bf49e8d-2e30-4454-8042-b7ab0b3ea9df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dnth/anaconda3/envs/pytorch-edge/lib/python3.10/site-packages/torch/onnx/_internal/jit_utils.py:306: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "/home/dnth/anaconda3/envs/pytorch-edge/lib/python3.10/site-packages/torch/onnx/utils.py:689: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dnth/anaconda3/envs/pytorch-edge/lib/python3.10/site-packages/torch/onnx/utils.py:1186: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    }
   ],
   "source": [
    "import torch.onnx\n",
    "torch.onnx.export(model,\n",
    "                 torch.rand(1, 3, 224, 224, requires_grad=True),\n",
    "                 \"efficientvit_b0.r224_in1k.onnx\",\n",
    "                 export_params=True,\n",
    "                 opset_version=16,\n",
    "                 do_constant_folding=True,\n",
    "                 input_names=['input'],\n",
    "                 output_names=['output'], \n",
    "                 dynamic_axes={'input' : {0 : 'batch_size'},   \n",
    "                               'output' : {0 : 'batch_size'}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0175bff3-8fef-4706-a06d-da5ba3c42471",
   "metadata": {},
   "source": [
    "## ONNX Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e4d9c2bd-dcc6-451c-b5c0-f23c97d9a5b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from PIL import Image\n",
    "from urllib.request import urlopen\n",
    "\n",
    "#define the priority order for the execution providers\n",
    "\n",
    "# prefer CUDA Execution Provider over CPU Execution Provider\n",
    "EP_list = ['CUDAExecutionProvider', 'CPUExecutionProvider', 'OpenVINOExecutionProvider']\n",
    "\n",
    "# Load ONNX model\n",
    "session = ort.InferenceSession(\"efficientvit_b0.r224_in1k.onnx\", providers=EP_list)\n",
    "\n",
    "session.set_providers(['CPUExecutionProvider'])\n",
    "\n",
    "# Load an image\n",
    "img = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\n",
    "img = img.convert('RGB')\n",
    "img = img.resize((224, 224))\n",
    "img_np = np.array(img).astype(np.float32)\n",
    "\n",
    "# Convert data to the shape the ONNX model expects\n",
    "input_data = np.transpose(img_np, (2, 0, 1))  # Convert to (C, H, W)\n",
    "input_data = np.expand_dims(input_data, axis=0)  # Add a batch dimension\n",
    "\n",
    "input_data.shape\n",
    "\n",
    "# Get input name from the model\n",
    "input_name = session.get_inputs()[0].name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "da39cf5f-7d72-4fce-9b2c-40a1840afb7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.62 ms ± 401 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# Perform inference\n",
    "output = session.run(None, {input_name: input_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "8bf94c32-23eb-4112-9568-1727245ee987",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract output data (assuming model has a single output)\n",
    "output_data = output[0]\n",
    "\n",
    "output_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5194ed-24e6-4b46-a8e1-a133e481e937",
   "metadata": {},
   "source": [
    "## Visualize Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cdb70f92-49a4-492d-ada4-4696b7d50745",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -Uq netron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8bc4b4e1-8047-448b-8905-50540edcccd7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'efficientvit_b0.r224_in1k.onnx' at http://localhost:6006\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"1000\"\n",
       "            src=\"http://localhost:6006\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fa30c193940>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "import netron\n",
    "port = 6006\n",
    "model_path = \"efficientvit_b0.r224_in1k.onnx\"\n",
    "netron.start(model_path, 6006, browse=False)\n",
    "\n",
    "IPython.display.IFrame(f\"http://localhost:{port}\", width=1000, height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "976123a1-ef4b-4c4d-8488-9d72197d04d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -Uq onnxsim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f221ebff-0777-40dd-b6ce-7fc878bf5436",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mYour model contains \"Tile\" ops or/and \"ConstantOfShape\" ops. Folding these ops \u001b[0m\n",
      "\u001b[1;35mcan make the simplified model much larger. If it is not expected, please specify\u001b[0m\n",
      "\u001b[1;35m\"--no-large-tensor\" (which will lose some optimization chances)\u001b[0m\n",
      "Simplifying\u001b[33m...\u001b[0m\n",
      "Finish! Here is the difference:\n",
      "┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOriginal Model\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSimplified Model\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
      "│ Add               │ 21             │ 21               │\n",
      "│ Cast              │ 20             │ \u001b[1;38;5;46m0               \u001b[0m │\n",
      "│ Concat            │ 16             │ \u001b[1;38;5;46m12              \u001b[0m │\n",
      "│ Constant          │ 239            │ \u001b[1;38;5;46m109             \u001b[0m │\n",
      "│ ConstantOfShape   │ 4              │ \u001b[1;38;5;46m0               \u001b[0m │\n",
      "│ Conv              │ 50             │ 50               │\n",
      "│ Div               │ 9              │ 9                │\n",
      "│ Flatten           │ 1              │ 1                │\n",
      "│ Gather            │ 16             │ \u001b[1;38;5;46m14              \u001b[0m │\n",
      "│ Gemm              │ 1              │ 1                │\n",
      "│ GlobalAveragePool │ 1              │ 1                │\n",
      "│ HardSwish         │ 24             │ 24               │\n",
      "│ MatMul            │ 9              │ 9                │\n",
      "│ Mul               │ 17             │ \u001b[1;38;5;46m12              \u001b[0m │\n",
      "│ Pad               │ 4              │ 4                │\n",
      "│ Pow               │ 1              │ 1                │\n",
      "│ ReduceMean        │ 2              │ 2                │\n",
      "│ Relu              │ 8              │ 8                │\n",
      "│ Reshape           │ 16             │ \u001b[1;38;5;46m8               \u001b[0m │\n",
      "│ Shape             │ 16             │ \u001b[1;38;5;46m8               \u001b[0m │\n",
      "│ Slice             │ 24             │ \u001b[1;38;5;46m20              \u001b[0m │\n",
      "│ Sqrt              │ 1              │ 1                │\n",
      "│ Sub               │ 1              │ 1                │\n",
      "│ Transpose         │ 16             │ \u001b[1;38;5;46m12              \u001b[0m │\n",
      "│ Unsqueeze         │ 20             │ \u001b[1;38;5;46m13              \u001b[0m │\n",
      "│ Model Size        │ 13.1MiB        │ \u001b[1;38;5;46m13.1MiB         \u001b[0m │\n",
      "└───────────────────┴────────────────┴──────────────────┘\n"
     ]
    }
   ],
   "source": [
    "!onnxsim efficientvit_b0.r224_in1k.onnx efficientvit_b0.r224_in1k_simplified.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f2db9f75-8b6a-4db3-a6bc-a6f0c41364aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'efficientvit_b0.r224_in1k_simplified.onnx' at http://localhost:6009\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"1000\"\n",
       "            src=\"http://localhost:6006\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fa1b0efba30>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import netron\n",
    "port = 6006\n",
    "model_path = \"efficientvit_b0.r224_in1k_simplified.onnx\"\n",
    "netron.start(model_path, 6009, browse=False)\n",
    "\n",
    "IPython.display.IFrame(f\"http://localhost:{port}\", width=1000, height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "a788c4cd-e178-48b5-a2dc-b60e2c4a91ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from PIL import Image\n",
    "from urllib.request import urlopen\n",
    "\n",
    "#define the priority order for the execution providers\n",
    "\n",
    "# prefer CUDA Execution Provider over CPU Execution Provider\n",
    "EP_list = ['CUDAExecutionProvider', 'CPUExecutionProvider', 'OpenVINOExecutionProvider']\n",
    "\n",
    "# Load ONNX model\n",
    "session = ort.InferenceSession(\"efficientvit_b0.r224_in1k_simplified.onnx\", providers=EP_list)\n",
    "\n",
    "session.set_providers(['CPUExecutionProvider'])\n",
    "\n",
    "# Load an image\n",
    "img = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\n",
    "img = img.convert('RGB')\n",
    "img = img.resize((224, 224))\n",
    "img_np = np.array(img).astype(np.float32)\n",
    "\n",
    "# Convert data to the shape the ONNX model expects\n",
    "input_data = np.transpose(img_np, (2, 0, 1))  # Convert to (C, H, W)\n",
    "input_data = np.expand_dims(input_data, axis=0)  # Add a batch dimension\n",
    "\n",
    "input_data.shape\n",
    "\n",
    "# Get input name from the model\n",
    "input_name = session.get_inputs()[0].name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "7452c540-37bf-4f35-94f8-a6006073e3a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.61 ms ± 286 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# Perform inference\n",
    "output = session.run(None, {input_name: input_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894ec0bc-bee6-488d-b22d-814d4df2a6f1",
   "metadata": {},
   "source": [
    "## Quantize ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "411d44d3-e76c-42e1-9270-bcdf47597349",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/stages/stages.2/blocks/blocks.1/context_module/main/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/stages/stages.2/blocks/blocks.1/context_module/main/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/stages/stages.2/blocks/blocks.2/context_module/main/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/stages/stages.2/blocks/blocks.2/context_module/main/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/stages/stages.3/blocks/blocks.1/context_module/main/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/stages/stages.3/blocks/blocks.1/context_module/main/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/stages/stages.3/blocks/blocks.2/context_module/main/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/stages/stages.3/blocks/blocks.2/context_module/main/MatMul_1]\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "model_fp32 = 'efficientvit_b0.r224_in1k_simplified.onnx'\n",
    "model_quant = 'efficientvit_b0.r224_in1k_simplified_quantized.onnx'\n",
    "quantized_model = quantize_dynamic(model_fp32, model_quant)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0de9bccc-9ce0-41f4-bed0-9fdd81002997",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from PIL import Image\n",
    "from urllib.request import urlopen\n",
    "\n",
    "#define the priority order for the execution providers\n",
    "\n",
    "# prefer CUDA Execution Provider over CPU Execution Provider\n",
    "EP_list = ['CUDAExecutionProvider', 'CPUExecutionProvider', 'OpenVINOExecutionProvider']\n",
    "\n",
    "# Load ONNX model\n",
    "session = ort.InferenceSession(\"efficientvit_b0.r224_in1k_simplified_quantized_static.onnx\", providers=EP_list)\n",
    "\n",
    "# session.set_providers(['CPUExecutionProvider'])\n",
    "\n",
    "# Load an image\n",
    "img = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\n",
    "img = img.convert('RGB')\n",
    "img = img.resize((224, 224))\n",
    "img_np = np.array(img).astype(np.float32)\n",
    "\n",
    "# Convert data to the shape the ONNX model expects\n",
    "input_data = np.transpose(img_np, (2, 0, 1))  # Convert to (C, H, W)\n",
    "input_data = np.expand_dims(input_data, axis=0)  # Add a batch dimension\n",
    "\n",
    "input_data.shape\n",
    "\n",
    "# Get input name from the model\n",
    "input_name = session.get_inputs()[0].name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "be8d16b0-fd69-47c7-9428-ab495612fe7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.3 ms ± 2.55 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# Perform inference\n",
    "output = session.run(None, {input_name: input_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75b777f-6dd0-4a7a-b3d3-0f3eacb5b9ce",
   "metadata": {},
   "source": [
    "## ONNX to OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2227cf7d-552f-409f-9908-c7f38bc0dfbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -Uq openvino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f44e68c6-345b-4632-973f-8c0e90bd7a6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "ov_model = ov.convert_model('efficientvit_b0.r224_in1k_simplified.onnx')\n",
    "\n",
    "###### Option 1: Save to OpenVINO IR:\n",
    "\n",
    "# save model to OpenVINO IR for later use\n",
    "ov.save_model(ov_model, 'efficientvit_b0.r224_in1k_simplified.onnx.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "75af8458-2c08-498f-b814-c99b3c3bf8f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load an image\n",
    "img = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\n",
    "img = img.convert('RGB')\n",
    "img = img.resize((224, 224))\n",
    "img_np = np.array(img).astype(np.float32)\n",
    "\n",
    "# Convert data to the shape the ONNX model expects\n",
    "input_data = np.transpose(img_np, (2, 0, 1))  # Convert to (C, H, W)\n",
    "input_data = np.expand_dims(input_data, axis=0)  # Add a batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "dea62c78-ec97-4dd3-a9b9-a3d48c9b6776",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 224, 224)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "4dc7a5bc-aa76-4ad7-b47b-2f5d40b5839d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###### Option 2: Compile and infer with OpenVINO:\n",
    "\n",
    "# compile model\n",
    "compiled_model = ov.compile_model(ov_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "5a5646cb-4f44-4b75-a4e8-2509c2393f7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.42 ms ± 242 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# run inference\n",
    "result = compiled_model(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afdb088-0cc0-4099-be15-2d3676c46bee",
   "metadata": {},
   "source": [
    "## PyTorch to OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "72a59cc9-b9af-42c8-997b-d456276fbe12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "import timm\n",
    "import torch\n",
    "\n",
    "img = Image.open(urlopen(\n",
    "    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n",
    "))\n",
    "\n",
    "model = timm.create_model('efficientvit_b0.r224_in1k', pretrained=True)\n",
    "model = model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "d5db3c7e-b7a9-4e96-a319-e727887cd3ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "\n",
    "# Create OpenVINO Core object instance\n",
    "core = ov.Core()\n",
    "\n",
    "# Convert model to openvino.runtime.Model object\n",
    "ov_model = ov.convert_model(model)\n",
    "\n",
    "MODEL_NAME = 'efficientvit_b0.r224_in1k'\n",
    "\n",
    "# Save openvino.runtime.Model object on disk\n",
    "ov.save_model(ov_model, f\"{MODEL_NAME}_dynamic.xml\")\n",
    "\n",
    "# Load OpenVINO model on device\n",
    "compiled_model = core.compile_model(ov_model, 'AUTO')\n",
    "\n",
    "input_tensor=transforms(img).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "b0bf7767-d019-4f25-b671-0db4f8901c4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.37 ms ± 128 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "result = compiled_model(input_tensor)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "aef376f6-ed6b-446a-a976-0053cd2d43a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1000)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ddaed8-47d9-4efe-8d24-cb4a3670f6ce",
   "metadata": {},
   "source": [
    "## PyTorch to Torchscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "73d09fe7-ee09-4dbb-8f65-e7c7b88eb0ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "\n",
    "model.eval()\n",
    "example = torch.rand(1, 3, 224, 224)\n",
    "traced_script_module = torch.jit.trace(model, example)\n",
    "optimized_traced_model = optimize_for_mobile(traced_script_module)\n",
    "optimized_traced_model._save_for_lite_interpreter(\"torchscript_efficientvit_b0.r224_in1k.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6325e75c-267b-4601-ac6a-948474dc22a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Step 1: Load the TorchScript model\n",
    "model = torch.jit.load(\"torchscript_efficientvit_b0.r224_in1k.pt\")\n",
    "\n",
    "# Step 2: Prepare input data\n",
    "# Assuming the model expects a 1D tensor of size 10 as input\n",
    "# Load an image\n",
    "img = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\n",
    "img = img.convert('RGB')\n",
    "img = img.resize((224, 224))\n",
    "img_np = np.array(img).astype(np.float32)\n",
    "\n",
    "# Convert data to the shape the ONNX model expects\n",
    "input_data = np.transpose(img_np, (2, 0, 1))  # Convert to (C, H, W)\n",
    "input_data = np.expand_dims(input_data, axis=0)  # Add a batch dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "71f4bf4e-fd29-47df-b02d-abb1253dc012",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 224, 224)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "005200b7-50ae-46cb-be48-e11f5790fbdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_tensor = torch.tensor(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2cd94847-5c65-452a-932c-77b10bcdc871",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8 s ± 136 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "# Step 3: Run inference\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "296f01f3-e2bd-4abb-90c7-7e175e527950",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 4: Process output\n",
    "# Convert to NumPy array or perform other operations\n",
    "output_array = output.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b4fe358b-ff25-42eb-9503-c677dc428c9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1000)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8c987a09-1611-4124-b961-8e60e8d19fdf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(original_name=EfficientVit)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308c1e89-b280-443a-991f-fd96cbb32538",
   "metadata": {},
   "source": [
    "## PyTorch to OpenVINO - torch.compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d9e780be-de77-49a3-9093-39d97e9aa971",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openvino.torch\n",
    "model = torch.compile(model, backend='openvino')\n",
    "# OR\n",
    "# model = torch.compile(model, backend='openvino_ts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "67b37fe5-4cef-47d4-bf71-dc9801f9814a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): EfficientVit(\n",
       "    (stem): Stem(\n",
       "      (in_conv): ConvNormAct(\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv): Conv2d(3, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (norm): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): Hardswish()\n",
       "      )\n",
       "      (res0): ResidualBlock(\n",
       "        (pre_norm): Identity()\n",
       "        (main): DSConv(\n",
       "          (depth_conv): ConvNormAct(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
       "            (norm): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): Hardswish()\n",
       "          )\n",
       "          (point_conv): ConvNormAct(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): Identity()\n",
       "          )\n",
       "        )\n",
       "        (shortcut): Identity()\n",
       "      )\n",
       "    )\n",
       "    (stages): Sequential(\n",
       "      (0): EfficientVitStage(\n",
       "        (blocks): Sequential(\n",
       "          (0): ResidualBlock(\n",
       "            (pre_norm): Identity()\n",
       "            (main): MBConv(\n",
       "              (inverted_conv): ConvNormAct(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (conv): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act): Hardswish()\n",
       "              )\n",
       "              (depth_conv): ConvNormAct(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "                (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act): Hardswish()\n",
       "              )\n",
       "              (point_conv): ConvNormAct(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (conv): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): ResidualBlock(\n",
       "            (pre_norm): Identity()\n",
       "            (main): MBConv(\n",
       "              (inverted_conv): ConvNormAct(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (conv): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act): Hardswish()\n",
       "              )\n",
       "              (depth_conv): ConvNormAct(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "                (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act): Hardswish()\n",
       "              )\n",
       "              (point_conv): ConvNormAct(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "            (shortcut): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): EfficientVitStage(\n",
       "        (blocks): Sequential(\n",
       "          (0): ResidualBlock(\n",
       "            (pre_norm): Identity()\n",
       "            (main): MBConv(\n",
       "              (inverted_conv): ConvNormAct(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (conv): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act): Hardswish()\n",
       "              )\n",
       "              (depth_conv): ConvNormAct(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
       "                (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act): Hardswish()\n",
       "              )\n",
       "              (point_conv): ConvNormAct(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): ResidualBlock(\n",
       "            (pre_norm): Identity()\n",
       "            (main): MBConv(\n",
       "              (inverted_conv): ConvNormAct(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (conv): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act): Hardswish()\n",
       "              )\n",
       "              (depth_conv): ConvNormAct(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "                (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act): Hardswish()\n",
       "              )\n",
       "              (point_conv): ConvNormAct(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "            (shortcut): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): EfficientVitStage(\n",
       "        (blocks): Sequential(\n",
       "          (0): ResidualBlock(\n",
       "            (pre_norm): Identity()\n",
       "            (main): MBConv(\n",
       "              (inverted_conv): ConvNormAct(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (conv): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (norm): Identity()\n",
       "                (act): Hardswish()\n",
       "              )\n",
       "              (depth_conv): ConvNormAct(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128)\n",
       "                (norm): Identity()\n",
       "                (act): Hardswish()\n",
       "              )\n",
       "              (point_conv): ConvNormAct(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): EfficientVitBlock(\n",
       "            (context_module): ResidualBlock(\n",
       "              (pre_norm): Identity()\n",
       "              (main): LiteMSA(\n",
       "                (qkv): ConvNormAct(\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (aggreg): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)\n",
       "                    (1): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), groups=12, bias=False)\n",
       "                  )\n",
       "                )\n",
       "                (kernel_func): ReLU()\n",
       "                (proj): ConvNormAct(\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (act): Identity()\n",
       "                )\n",
       "              )\n",
       "              (shortcut): Identity()\n",
       "            )\n",
       "            (local_module): ResidualBlock(\n",
       "              (pre_norm): Identity()\n",
       "              (main): MBConv(\n",
       "                (inverted_conv): ConvNormAct(\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (norm): Identity()\n",
       "                  (act): Hardswish()\n",
       "                )\n",
       "                (depth_conv): ConvNormAct(\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                  (norm): Identity()\n",
       "                  (act): Hardswish()\n",
       "                )\n",
       "                (point_conv): ConvNormAct(\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (act): Identity()\n",
       "                )\n",
       "              )\n",
       "              (shortcut): Identity()\n",
       "            )\n",
       "          )\n",
       "          (2): EfficientVitBlock(\n",
       "            (context_module): ResidualBlock(\n",
       "              (pre_norm): Identity()\n",
       "              (main): LiteMSA(\n",
       "                (qkv): ConvNormAct(\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (aggreg): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)\n",
       "                    (1): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), groups=12, bias=False)\n",
       "                  )\n",
       "                )\n",
       "                (kernel_func): ReLU()\n",
       "                (proj): ConvNormAct(\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (act): Identity()\n",
       "                )\n",
       "              )\n",
       "              (shortcut): Identity()\n",
       "            )\n",
       "            (local_module): ResidualBlock(\n",
       "              (pre_norm): Identity()\n",
       "              (main): MBConv(\n",
       "                (inverted_conv): ConvNormAct(\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (norm): Identity()\n",
       "                  (act): Hardswish()\n",
       "                )\n",
       "                (depth_conv): ConvNormAct(\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                  (norm): Identity()\n",
       "                  (act): Hardswish()\n",
       "                )\n",
       "                (point_conv): ConvNormAct(\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (act): Identity()\n",
       "                )\n",
       "              )\n",
       "              (shortcut): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): EfficientVitStage(\n",
       "        (blocks): Sequential(\n",
       "          (0): ResidualBlock(\n",
       "            (pre_norm): Identity()\n",
       "            (main): MBConv(\n",
       "              (inverted_conv): ConvNormAct(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (norm): Identity()\n",
       "                (act): Hardswish()\n",
       "              )\n",
       "              (depth_conv): ConvNormAct(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
       "                (norm): Identity()\n",
       "                (act): Hardswish()\n",
       "              )\n",
       "              (point_conv): ConvNormAct(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): EfficientVitBlock(\n",
       "            (context_module): ResidualBlock(\n",
       "              (pre_norm): Identity()\n",
       "              (main): LiteMSA(\n",
       "                (qkv): ConvNormAct(\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (aggreg): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "                    (1): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), groups=24, bias=False)\n",
       "                  )\n",
       "                )\n",
       "                (kernel_func): ReLU()\n",
       "                (proj): ConvNormAct(\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (act): Identity()\n",
       "                )\n",
       "              )\n",
       "              (shortcut): Identity()\n",
       "            )\n",
       "            (local_module): ResidualBlock(\n",
       "              (pre_norm): Identity()\n",
       "              (main): MBConv(\n",
       "                (inverted_conv): ConvNormAct(\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (norm): Identity()\n",
       "                  (act): Hardswish()\n",
       "                )\n",
       "                (depth_conv): ConvNormAct(\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                  (norm): Identity()\n",
       "                  (act): Hardswish()\n",
       "                )\n",
       "                (point_conv): ConvNormAct(\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (act): Identity()\n",
       "                )\n",
       "              )\n",
       "              (shortcut): Identity()\n",
       "            )\n",
       "          )\n",
       "          (2): EfficientVitBlock(\n",
       "            (context_module): ResidualBlock(\n",
       "              (pre_norm): Identity()\n",
       "              (main): LiteMSA(\n",
       "                (qkv): ConvNormAct(\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (aggreg): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "                    (1): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), groups=24, bias=False)\n",
       "                  )\n",
       "                )\n",
       "                (kernel_func): ReLU()\n",
       "                (proj): ConvNormAct(\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (act): Identity()\n",
       "                )\n",
       "              )\n",
       "              (shortcut): Identity()\n",
       "            )\n",
       "            (local_module): ResidualBlock(\n",
       "              (pre_norm): Identity()\n",
       "              (main): MBConv(\n",
       "                (inverted_conv): ConvNormAct(\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (norm): Identity()\n",
       "                  (act): Hardswish()\n",
       "                )\n",
       "                (depth_conv): ConvNormAct(\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                  (norm): Identity()\n",
       "                  (act): Hardswish()\n",
       "                )\n",
       "                (point_conv): ConvNormAct(\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (act): Identity()\n",
       "                )\n",
       "              )\n",
       "              (shortcut): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): ClassifierHead(\n",
       "      (in_conv): ConvNormAct(\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): Hardswish()\n",
       "      )\n",
       "      (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
       "      (classifier): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        (1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): Hardswish()\n",
       "        (3): Dropout(p=0.0, inplace=False)\n",
       "        (4): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "28e7fde8-0ce7-4728-a9e1-e62f9cea20a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get model specific transforms (normalization, resize)\n",
    "data_config = timm.data.resolve_model_data_config(model)\n",
    "transforms = timm.data.create_transform(**data_config, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b6df3c00-046d-4cd5-9621-7c3cfec3b411",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.32 ms ± 455 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accc2118-f70a-4eb3-86af-2377dd0f6736",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
