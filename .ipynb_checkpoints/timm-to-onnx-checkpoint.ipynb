{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81cdcd5a-fd74-4bb9-9ad8-af94de238e2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -Uq timm onnx onnxruntime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccb9bbd-f87e-4573-b8b8-2003946ab517",
   "metadata": {},
   "source": [
    "## PyTorch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8d290ce0-e6b6-4724-9223-7668fed22f22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dnth/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "import timm\n",
    "import torch\n",
    "\n",
    "img = Image.open(urlopen(\n",
    "    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n",
    "))\n",
    "\n",
    "model = timm.create_model('efficientvit_b0.r224_in1k', pretrained=True)\n",
    "model = model.eval()\n",
    "\n",
    "# get model specific transforms (normalization, resize)\n",
    "data_config = timm.data.resolve_model_data_config(model)\n",
    "transforms = timm.data.create_transform(**data_config, is_training=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a376225-5843-47e2-80f6-9b7ac8ebe3ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 69.1 ms, sys: 0 ns, total: 69.1 ms\n",
      "Wall time: 12.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7b4dbc4-25f0-40ae-90cf-a11245a02d08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e3105dd-7417-4f9a-99b1-b044c9527ecc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[42.1539,  9.7981,  9.3126,  5.4653,  4.5588]],\n",
       "       grad_fn=<TopkBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc7270a1-c284-456e-9277-2681fe68a2b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[928, 551, 969, 967, 505]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5_class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1166b0-e6fd-4c13-8e27-78b5637196ea",
   "metadata": {},
   "source": [
    "## Convert To ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34b09639-41ee-4f4f-971b-f9bf2b5ced51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from timm.utils.model import reparameterize_model\n",
    "model = reparameterize_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bf49e8d-2e30-4454-8042-b7ab0b3ea9df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.onnx\n",
    "torch.onnx.export(model,\n",
    "                 torch.rand(1, 3, 224, 224, requires_grad=True),\n",
    "                 \"efficientvit_b0.r224_in1k.onnx\",\n",
    "                 export_params=True,\n",
    "                 opset_version=16,\n",
    "                 do_constant_folding=True,\n",
    "                 input_names=['input'],\n",
    "                 output_names=['output'], \n",
    "                 dynamic_axes={'input' : {0 : 'batch_size'},   \n",
    "                               'output' : {0 : 'batch_size'}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0175bff3-8fef-4706-a06d-da5ba3c42471",
   "metadata": {},
   "source": [
    "## ONNX Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e4d9c2bd-dcc6-451c-b5c0-f23c97d9a5b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from PIL import Image\n",
    "from urllib.request import urlopen\n",
    "\n",
    "#define the priority order for the execution providers\n",
    "\n",
    "# prefer CUDA Execution Provider over CPU Execution Provider\n",
    "EP_list = ['CUDAExecutionProvider', 'CPUExecutionProvider', 'OpenVINOExecutionProvider']\n",
    "\n",
    "# Load ONNX model\n",
    "session = ort.InferenceSession(\"efficientvit_b0.r224_in1k.onnx\", providers=EP_list)\n",
    "\n",
    "session.set_providers(['CPUExecutionProvider'])\n",
    "\n",
    "# Load an image\n",
    "img = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\n",
    "img = img.convert('RGB')\n",
    "img = img.resize((224, 224))\n",
    "img_np = np.array(img).astype(np.float32)\n",
    "\n",
    "# Convert data to the shape the ONNX model expects\n",
    "input_data = np.transpose(img_np, (2, 0, 1))  # Convert to (C, H, W)\n",
    "input_data = np.expand_dims(input_data, axis=0)  # Add a batch dimension\n",
    "\n",
    "input_data.shape\n",
    "\n",
    "# Get input name from the model\n",
    "input_name = session.get_inputs()[0].name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da39cf5f-7d72-4fce-9b2c-40a1840afb7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.85 ms ± 84.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# Perform inference\n",
    "output = session.run(None, {input_name: input_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8bf94c32-23eb-4112-9568-1727245ee987",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1000)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract output data (assuming model has a single output)\n",
    "output_data = output[0]\n",
    "\n",
    "output_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5194ed-24e6-4b46-a8e1-a133e481e937",
   "metadata": {},
   "source": [
    "## Visualize Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cdb70f92-49a4-492d-ada4-4696b7d50745",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -Uq netron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8bc4b4e1-8047-448b-8905-50540edcccd7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'efficientvit_b0.r224_in1k.onnx' at http://localhost:6006\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"1000\"\n",
       "            src=\"http://localhost:6006\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fa30c193940>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "import netron\n",
    "port = 6006\n",
    "model_path = \"efficientvit_b0.r224_in1k.onnx\"\n",
    "netron.start(model_path, 6006, browse=False)\n",
    "\n",
    "IPython.display.IFrame(f\"http://localhost:{port}\", width=1000, height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "976123a1-ef4b-4c4d-8488-9d72197d04d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -Uq onnxsim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f221ebff-0777-40dd-b6ce-7fc878bf5436",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mYour model contains \"Tile\" ops or/and \"ConstantOfShape\" ops. Folding these ops \u001b[0m\n",
      "\u001b[1;35mcan make the simplified model much larger. If it is not expected, please specify\u001b[0m\n",
      "\u001b[1;35m\"--no-large-tensor\" (which will lose some optimization chances)\u001b[0m\n",
      "Simplifying\u001b[33m...\u001b[0m\n",
      "Finish! Here is the difference:\n",
      "┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOriginal Model\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSimplified Model\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
      "│ Add               │ 21             │ 21               │\n",
      "│ Cast              │ 20             │ \u001b[1;38;5;46m0               \u001b[0m │\n",
      "│ Concat            │ 16             │ \u001b[1;38;5;46m12              \u001b[0m │\n",
      "│ Constant          │ 239            │ \u001b[1;38;5;46m109             \u001b[0m │\n",
      "│ ConstantOfShape   │ 4              │ \u001b[1;38;5;46m0               \u001b[0m │\n",
      "│ Conv              │ 50             │ 50               │\n",
      "│ Div               │ 9              │ 9                │\n",
      "│ Flatten           │ 1              │ 1                │\n",
      "│ Gather            │ 16             │ \u001b[1;38;5;46m14              \u001b[0m │\n",
      "│ Gemm              │ 1              │ 1                │\n",
      "│ GlobalAveragePool │ 1              │ 1                │\n",
      "│ HardSwish         │ 24             │ 24               │\n",
      "│ MatMul            │ 9              │ 9                │\n",
      "│ Mul               │ 17             │ \u001b[1;38;5;46m12              \u001b[0m │\n",
      "│ Pad               │ 4              │ 4                │\n",
      "│ Pow               │ 1              │ 1                │\n",
      "│ ReduceMean        │ 2              │ 2                │\n",
      "│ Relu              │ 8              │ 8                │\n",
      "│ Reshape           │ 16             │ \u001b[1;38;5;46m8               \u001b[0m │\n",
      "│ Shape             │ 16             │ \u001b[1;38;5;46m8               \u001b[0m │\n",
      "│ Slice             │ 24             │ \u001b[1;38;5;46m20              \u001b[0m │\n",
      "│ Sqrt              │ 1              │ 1                │\n",
      "│ Sub               │ 1              │ 1                │\n",
      "│ Transpose         │ 16             │ \u001b[1;38;5;46m12              \u001b[0m │\n",
      "│ Unsqueeze         │ 20             │ \u001b[1;38;5;46m13              \u001b[0m │\n",
      "│ Model Size        │ 13.1MiB        │ \u001b[1;38;5;46m13.1MiB         \u001b[0m │\n",
      "└───────────────────┴────────────────┴──────────────────┘\n"
     ]
    }
   ],
   "source": [
    "!onnxsim efficientvit_b0.r224_in1k.onnx efficientvit_b0.r224_in1k_simplified.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f2db9f75-8b6a-4db3-a6bc-a6f0c41364aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://localhost:6006\n",
      "Serving 'efficientvit_b0.r224_in1k_simplified.onnx' at http://localhost:6006\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"1000\"\n",
       "            src=\"http://localhost:6006\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fa2e055d840>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import netron\n",
    "port = 6006\n",
    "model_path = \"efficientvit_b0.r224_in1k_simplified.onnx\"\n",
    "netron.start(model_path, 6006, browse=False)\n",
    "\n",
    "IPython.display.IFrame(f\"http://localhost:{port}\", width=1000, height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a788c4cd-e178-48b5-a2dc-b60e2c4a91ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from PIL import Image\n",
    "from urllib.request import urlopen\n",
    "\n",
    "#define the priority order for the execution providers\n",
    "\n",
    "# prefer CUDA Execution Provider over CPU Execution Provider\n",
    "EP_list = ['CUDAExecutionProvider', 'CPUExecutionProvider', 'OpenVINOExecutionProvider']\n",
    "\n",
    "# Load ONNX model\n",
    "session = ort.InferenceSession(\"efficientvit_b0.r224_in1k_simplified.onnx\", providers=EP_list)\n",
    "\n",
    "session.set_providers(['CPUExecutionProvider'])\n",
    "\n",
    "# Load an image\n",
    "img = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\n",
    "img = img.convert('RGB')\n",
    "img = img.resize((224, 224))\n",
    "img_np = np.array(img).astype(np.float32)\n",
    "\n",
    "# Convert data to the shape the ONNX model expects\n",
    "input_data = np.transpose(img_np, (2, 0, 1))  # Convert to (C, H, W)\n",
    "input_data = np.expand_dims(input_data, axis=0)  # Add a batch dimension\n",
    "\n",
    "input_data.shape\n",
    "\n",
    "# Get input name from the model\n",
    "input_name = session.get_inputs()[0].name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7452c540-37bf-4f35-94f8-a6006073e3a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.01 ms ± 17.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# Perform inference\n",
    "output = session.run(None, {input_name: input_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75b777f-6dd0-4a7a-b3d3-0f3eacb5b9ce",
   "metadata": {},
   "source": [
    "## ONNX to OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2227cf7d-552f-409f-9908-c7f38bc0dfbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -Uq openvino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f44e68c6-345b-4632-973f-8c0e90bd7a6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "ov_model = ov.convert_model('efficientvit_b0.r224_in1k_simplified.onnx')\n",
    "\n",
    "###### Option 1: Save to OpenVINO IR:\n",
    "\n",
    "# save model to OpenVINO IR for later use\n",
    "ov.save_model(ov_model, 'efficientvit_b0.r224_in1k_simplified.onnx.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "75af8458-2c08-498f-b814-c99b3c3bf8f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load an image\n",
    "img = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\n",
    "img = img.convert('RGB')\n",
    "img = img.resize((224, 224))\n",
    "img_np = np.array(img).astype(np.float32)\n",
    "\n",
    "# Convert data to the shape the ONNX model expects\n",
    "input_data = np.transpose(img_np, (2, 0, 1))  # Convert to (C, H, W)\n",
    "input_data = np.expand_dims(input_data, axis=0)  # Add a batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dea62c78-ec97-4dd3-a9b9-a3d48c9b6776",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 224, 224)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4dc7a5bc-aa76-4ad7-b47b-2f5d40b5839d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###### Option 2: Compile and infer with OpenVINO:\n",
    "\n",
    "# compile model\n",
    "compiled_model = ov.compile_model(ov_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5a5646cb-4f44-4b75-a4e8-2509c2393f7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.07 ms ± 69 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# run inference\n",
    "result = compiled_model(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afdb088-0cc0-4099-be15-2d3676c46bee",
   "metadata": {},
   "source": [
    "## PyTorch to OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "72a59cc9-b9af-42c8-997b-d456276fbe12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "import timm\n",
    "import torch\n",
    "\n",
    "img = Image.open(urlopen(\n",
    "    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n",
    "))\n",
    "\n",
    "model = timm.create_model('efficientvit_b0.r224_in1k', pretrained=True)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d5db3c7e-b7a9-4e96-a319-e727887cd3ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "\n",
    "# Create OpenVINO Core object instance\n",
    "core = ov.Core()\n",
    "\n",
    "# Convert model to openvino.runtime.Model object\n",
    "ov_model = ov.convert_model(model)\n",
    "\n",
    "MODEL_NAME = 'efficientvit_b0.r224_in1k'\n",
    "\n",
    "# Save openvino.runtime.Model object on disk\n",
    "ov.save_model(ov_model, f\"{MODEL_NAME}_dynamic.xml\")\n",
    "\n",
    "# Load OpenVINO model on device\n",
    "compiled_model = core.compile_model(ov_model, 'AUTO')\n",
    "\n",
    "input_tensor=transforms(img).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b0bf7767-d019-4f25-b671-0db4f8901c4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.93 ms ± 48.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "result = compiled_model(input_tensor)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "aef376f6-ed6b-446a-a976-0053cd2d43a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1000)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ddaed8-47d9-4efe-8d24-cb4a3670f6ce",
   "metadata": {},
   "source": [
    "## PyTorch to Torchscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "73d09fe7-ee09-4dbb-8f65-e7c7b88eb0ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "\n",
    "model.eval()\n",
    "example = torch.rand(1, 3, 224, 224)\n",
    "traced_script_module = torch.jit.trace(model, example)\n",
    "optimized_traced_model = optimize_for_mobile(traced_script_module)\n",
    "optimized_traced_model._save_for_lite_interpreter(\"torchscript_efficientvit_b0.r224_in1k.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6325e75c-267b-4601-ac6a-948474dc22a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Step 1: Load the TorchScript model\n",
    "model = torch.jit.load(\"torchscript_efficientvit_b0.r224_in1k.pt\")\n",
    "\n",
    "# Step 2: Prepare input data\n",
    "# Assuming the model expects a 1D tensor of size 10 as input\n",
    "# Load an image\n",
    "img = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\n",
    "img = img.convert('RGB')\n",
    "img = img.resize((224, 224))\n",
    "img_np = np.array(img).astype(np.float32)\n",
    "\n",
    "# Convert data to the shape the ONNX model expects\n",
    "input_data = np.transpose(img_np, (2, 0, 1))  # Convert to (C, H, W)\n",
    "input_data = np.expand_dims(input_data, axis=0)  # Add a batch dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "71f4bf4e-fd29-47df-b02d-abb1253dc012",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 224, 224)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "005200b7-50ae-46cb-be48-e11f5790fbdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_tensor = torch.tensor(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2cd94847-5c65-452a-932c-77b10bcdc871",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8 s ± 136 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "# Step 3: Run inference\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "296f01f3-e2bd-4abb-90c7-7e175e527950",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 4: Process output\n",
    "# Convert to NumPy array or perform other operations\n",
    "output_array = output.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b4fe358b-ff25-42eb-9503-c677dc428c9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1000)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8c987a09-1611-4124-b961-8e60e8d19fdf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(original_name=EfficientVit)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308c1e89-b280-443a-991f-fd96cbb32538",
   "metadata": {},
   "source": [
    "## PyTorch to OpenVINO - torch.compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d9e780be-de77-49a3-9093-39d97e9aa971",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openvino.torch\n",
    "model = torch.compile(model, backend='openvino')\n",
    "# OR\n",
    "# model = torch.compile(model, backend='openvino_ts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "67b37fe5-4cef-47d4-bf71-dc9801f9814a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function timm.models.efficientvit_mit.EfficientVit.forward(x)>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "28e7fde8-0ce7-4728-a9e1-e62f9cea20a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get model specific transforms (normalization, resize)\n",
    "data_config = timm.data.resolve_model_data_config(model)\n",
    "transforms = timm.data.create_transform(**data_config, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b6df3c00-046d-4cd5-9621-7c3cfec3b411",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.1 ms ± 68.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accc2118-f70a-4eb3-86af-2377dd0f6736",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
